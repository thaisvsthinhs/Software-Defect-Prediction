{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import javalang\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] ant_tokens.xlsx saved at C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\ant_tokens.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# PHẦN 1: Extract tokens from Java files\n",
    "# ===============================\n",
    "def extract_tokens(node):\n",
    "    tokens = []\n",
    "    if isinstance(node, javalang.tree.MethodInvocation):\n",
    "        tokens.append(node.member)\n",
    "    elif isinstance(node, javalang.tree.ClassCreator):\n",
    "        tokens.append(node.type.name)\n",
    "    elif isinstance(node, javalang.tree.MethodDeclaration):\n",
    "        tokens.append(node.name)\n",
    "    elif isinstance(node, javalang.tree.ClassDeclaration):\n",
    "        tokens.append(node.name)\n",
    "    elif isinstance(node, javalang.tree.EnumDeclaration):\n",
    "        tokens.append(node.name)\n",
    "    elif isinstance(node, javalang.tree.IfStatement):\n",
    "        tokens.append(\"<IF>\")\n",
    "    elif isinstance(node, javalang.tree.WhileStatement):\n",
    "        tokens.append(\"<WHILE>\")\n",
    "    elif isinstance(node, javalang.tree.ForStatement):\n",
    "        tokens.append(\"<FOR>\")\n",
    "    elif isinstance(node, javalang.tree.ThrowStatement):\n",
    "        tokens.append(\"<THROW>\")\n",
    "    elif isinstance(node, javalang.tree.CatchClause):\n",
    "        tokens.append(\"<CATCH>\")\n",
    "    \n",
    "    for child in node.children:\n",
    "        if isinstance(child, list):\n",
    "            for item in child:\n",
    "                if isinstance(item, javalang.tree.Node):\n",
    "                    tokens.extend(extract_tokens(item))\n",
    "        elif isinstance(child, javalang.tree.Node):\n",
    "            tokens.extend(extract_tokens(child))\n",
    "    return tokens\n",
    "\n",
    "def parse_java_file_to_token_vector(file_path):\n",
    "    \"\"\"\n",
    "    Đọc nội dung file .java, parse thành AST, rồi trích xuất token vector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='cp1252') as f:\n",
    "            code = f.read()\n",
    "        tree = javalang.parse.parse(code)\n",
    "        token_vector = extract_tokens(tree)\n",
    "        return token_vector\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[LỖI] Không tìm thấy file: {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"[LỖI] Không parse được file: {file_path}, lý do: {e}\")\n",
    "        return []\n",
    "\n",
    "def build_java_path(java_name, base_path):\n",
    "    \"\"\"\n",
    "    Từ tên gói + lớp (vd: org.apache.tools.ant.input.PropertyFileInputHandler)\n",
    "    -> Tạo đường dẫn tới file .java.\n",
    "    \"\"\"\n",
    "    relative_path = java_name.replace('.', '/') + \".java\"\n",
    "    full_path = os.path.join(base_path, relative_path)\n",
    "    return full_path.replace('\\\\', '/')  \n",
    "\n",
    "def extract_tokens_and_save():\n",
    "    \"\"\"\n",
    "    Đọc file ant-1.6.csv, parse từng file Java dựa trên cột 'name',\n",
    "    trích xuất token vector và lưu kết quả ra file Excel (ant_tokens.xlsx).\n",
    "    \"\"\"\n",
    "    csv_path = r\"C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\ant-1.6.csv\"\n",
    "    base_path = r\"C:/Users/Acer/Desktop/2024/lab/DP-CNN/apache-ant-1.6.0-src/apache-ant-1.6.0/src/main\"\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    all_token_vectors = []\n",
    "    for i, row in df.iterrows():\n",
    "        java_name = row['name']\n",
    "        file_path = build_java_path(java_name, base_path)\n",
    "        token_vector = parse_java_file_to_token_vector(file_path)\n",
    "        all_token_vectors.append(token_vector)\n",
    "    \n",
    "    df_tokens = pd.DataFrame({\n",
    "        'name': df['name'],\n",
    "        'token_vector': all_token_vectors\n",
    "    })\n",
    "    \n",
    "    output_excel = r\"C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\ant_tokens.xlsx\"\n",
    "    df_tokens.to_excel(output_excel, index=False)\n",
    "    print(f\"[DONE] ant_tokens.xlsx saved at {output_excel}\")\n",
    "\n",
    "extract_tokens_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] final_vectors.csv saved at C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\final_vectors.csv\n",
      "[DONE] token2id_mapping.csv saved at C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\token2id_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# PHẦN 2: Mapping tokens to ID and applying padding/truncate\n",
    "# ===============================\n",
    "def pad_or_truncate(int_list, fixed_length=213):\n",
    "    \"\"\"\n",
    "    Nếu độ dài int_list < fixed_length, thêm 0 ở cuối.\n",
    "    Nếu độ dài int_list > fixed_length, cắt bớt ở cuối.\n",
    "    \"\"\"\n",
    "    if len(int_list) < fixed_length:\n",
    "        return int_list + [0] * (fixed_length - len(int_list))\n",
    "    else:\n",
    "        return int_list[:fixed_length]\n",
    "\n",
    "def map_tokens_and_save():\n",
    "    \"\"\"\n",
    "    xây dựng mapping token -> ID, chuyển thành integer vector, áp dụng padding/truncate,\n",
    "    và lưu ra file CSV (final_vectors.csv) cùng với mapping token2id trong token2id_mapping.csv.\n",
    "    \"\"\"\n",
    "    input_excel = r\"C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\ant_tokens.xlsx\"\n",
    "    output_csv = r\"C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\final_vectors.csv\"\n",
    "    mapping_csv = r\"C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\token2id_mapping.csv\"\n",
    "    \n",
    "    df = pd.read_excel(input_excel)\n",
    "    df['token_vector'] = df['token_vector'].apply(lambda x: ast.literal_eval(x)) #chuyển từ string list -> list\n",
    "    \n",
    "    all_tokens = set()\n",
    "    for tokens in df['token_vector']:\n",
    "        all_tokens.update(tokens)\n",
    "    \n",
    "    token2id = {}\n",
    "    current_id = 1\n",
    "    for token in sorted(all_tokens):  \n",
    "        token2id[token] = current_id\n",
    "        current_id += 1\n",
    "    \n",
    "    def convert_tokens_to_int(tokens):\n",
    "        return [token2id.get(t, 0) for t in tokens]\n",
    "    \n",
    "    df['int_vector'] = df['token_vector'].apply(convert_tokens_to_int)\n",
    "    df['int_vector_fixed'] = df['int_vector'].apply(lambda x: pad_or_truncate(x, fixed_length=213))\n",
    "    \n",
    "    df_final = df[['name', 'int_vector_fixed']].copy()\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "    print(f\"[DONE] final_vectors.csv saved at {output_csv}\")\n",
    "    \n",
    "    mapping_df = pd.DataFrame(list(token2id.items()), columns=['token', 'id'])\n",
    "    mapping_df.to_csv(mapping_csv, index=False)\n",
    "    print(f\"[DONE] token2id_mapping.csv saved at {mapping_csv}\")\n",
    "\n",
    "map_tokens_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] train.csv saved at C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\train.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# PHẦN 3: Merge với file ant-1.6.csv, chuyển đổi cột bug, nhân đôi dòng buggy, lưu train.csv\n",
    "# ===============================\n",
    "def merge_and_duplicate():\n",
    "    ant_csv = r\"C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\ant-1.6.csv\"\n",
    "    df_ant = pd.read_csv(ant_csv)\n",
    "    \n",
    "    def transform_bug_value(x):\n",
    "        if x == 0:\n",
    "            return \"clean\"\n",
    "        else:\n",
    "            return \"buggy\"\n",
    "    \n",
    "    df_ant['bug'] = df_ant['bug'].apply(transform_bug_value)\n",
    "    df_ant_minimal = df_ant[['name', 'bug']].copy()\n",
    "    \n",
    "    final_vectors_csv = r\"C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\final_vectors.csv\"\n",
    "    df_vectors = pd.read_csv(final_vectors_csv)\n",
    "    \n",
    "    df_merged = pd.merge(df_ant_minimal, df_vectors, on='name', how='inner')\n",
    "    \n",
    "    df_buggy = df_merged[df_merged['bug'] == 'buggy']\n",
    "    df_merged_doubled = pd.concat([df_merged, df_buggy], ignore_index=True)\n",
    "    \n",
    "    output_csv = r\"C:\\Users\\Acer\\Desktop\\2024\\lab\\DP-CNN\\train.csv\"\n",
    "    df_merged_doubled.to_csv(output_csv, index=False)\n",
    "    print(f\"[DONE] train.csv saved at {output_csv}\")\n",
    "\n",
    "merge_and_duplicate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
